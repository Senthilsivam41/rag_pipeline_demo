# ğŸ”’ 100% Private Local RAG (Retrieval-Augmented Generation)

A fully local, privacy-preserving RAG pipeline that processes PDFs and answers questions using **Ollama** (local LLM) and **HuggingFace embeddings** â€” all data stays on your machine, no cloud calls required (except during initial setup).

## ğŸ“‹ What This Code Does

- **Upload PDFs** through a Streamlit web interface
- **Embed documents locally** using HuggingFace's `all-MiniLM-L6-v2` model
- **Create a searchable vector database** using FAISS (in-memory)
- **Answer questions** using Ollama's local LLM (Llama 3.2 1B by default)
- **Cite sources** with page numbers from the original PDF

### Key Features
- âœ… **100% Private** â€” No data sent to external APIs
- âœ… **Free** â€” Uses open-source models
- âœ… **Fast** â€” Runs locally on your Mac CPU
- âœ… **Citation-aware** â€” Shows which pages answers come from

---

## ğŸ› ï¸ Prerequisites (macOS)

### 1. **Python 3.11+**
Check if installed:
```bash
python3 --version
```

If not installed, install via [Homebrew](https://brew.sh/):
```bash
brew install python@3.11
```

### 2. **Ollama** (Local LLM Runtime)
Download and install from: https://ollama.com/download/mac

Verify installation:
```bash
ollama --version
```

Pull the default model (Llama 3.2 1B):
```bash
ollama pull llama3.2:1b
```

Start Ollama server (runs in background):
```bash
ollama serve
```
It will be available at `http://localhost:11434`

### 3. **UV** (Python Package Manager - Optional but Recommended)
```bash
brew install uv
```

Or use `pip` if you prefer (comes with Python).

### 4. **Git** (Already on macOS, but verify)
```bash
git --version
```

---

## ğŸ“¦ Installation

### Step 1: Clone the Repository
```bash
git clone <repository-url>
cd rag_pipelines
```

### Step 2: Create Virtual Environment
**Using UV (recommended):**
```bash
uv venv .venv
source .venv/bin/activate
```

**Or using Python's venv:**
```bash
python3 -m venv .venv
source .venv/bin/activate
```

### Step 3: Install Dependencies
```bash
uv pip install -r pyproject.toml
```

Or with regular pip:
```bash
pip install -e .
```

This installs:
- `streamlit` â€” Web UI framework
- `langchain`, `langchain-core`, `langchain-community` â€” RAG framework
- `langchain-ollama` â€” Ollama LLM integration
- `faiss-cpu` â€” Vector database
- `pypdf` â€” PDF loading
- `langchain-text-splitters` â€” Document chunking

---

## ğŸš€ How to Run

### Prerequisites Check:
1. Ollama server is running:
   ```bash
   ollama serve
   ```
   (Run in a separate terminal)

2. Virtual environment is activated:
   ```bash
   source .venv/bin/activate
   ```

### Start the App:
```bash
uv run streamlit run app.py
```

Or with regular Python:
```bash
streamlit run app.py
```

The app will open in your browser at **`http://localhost:8501`**

### Using the App:
1. **Upload a PDF** using the sidebar uploader
2. **Wait** for the document to be indexed (shows spinner)
3. **Ask questions** in the chat input
4. **View answers** with cited source pages

---

## ğŸ“ Project Structure

```
rag_pipelines/
â”œâ”€â”€ app.py                    # Main Streamlit app (LOCAL RAG with Ollama)
â”œâ”€â”€ chatwithpdf.py            # Alternative: Cloud-based RAG (OpenAI)
â”œâ”€â”€ main.py                   # Utility functions or main entry point
â”œâ”€â”€ pyproject.toml            # Project dependencies
â”œâ”€â”€ README.md                 # This file
â””â”€â”€ .venv/                    # Virtual environment (created after setup)
```

### File Descriptions:

- **`app.py`** â€” The main Streamlit application
  - Uses Ollama for LLM (local, free)
  - Uses HuggingFace embeddings (local, free)
  - No API keys required

- **`chatwithpdf.py`** â€” Alternative implementation
  - Uses OpenAI for LLM and embeddings
  - Requires OpenAI API key
  - Better quality but costs money
  - Not used by default

---

## âš™ï¸ Configuration

### Change the Local Model:
Edit line 55 in `app.py`:
```python
llm = ChatOllama(model="llama3.2:1b", temperature=0)
```

Available models (pull them first):
```bash
ollama pull llama2
ollama pull mistral
ollama pull neural-chat
```

### Adjust Chunk Size:
Edit line 28 in `app.py`:
```python
text_splitter = RecursiveCharacterTextSplitter(chunk_size=800, chunk_overlap=100)
```
- Larger chunks = more context, slower search
- Smaller chunks = less context, faster search

### Change Temperature:
`temperature=0` â†’ Deterministic (same answer every time)
`temperature=1` â†’ Creative/varied answers

---

## ğŸ› Troubleshooting

### Issue: "Module not found" errors
**Solution:** Make sure virtual environment is activated:
```bash
source .venv/bin/activate
```

### Issue: "Connection refused" when running the app
**Solution:** Ollama server is not running. In another terminal:
```bash
ollama serve
```

### Issue: App is slow
**Solution:** 
- You're using a 1B model (lightweight). For better quality, use a larger model:
  ```bash
  ollama pull llama2
  ```
  Then update `app.py` line 55 to `model="llama2"`
- Close other applications to free up RAM

### Issue: PDF not uploading
**Solution:** 
- Ensure the PDF is under 100MB
- Try a different PDF file to test
- Check browser console for errors (F12)

---

## ğŸ“Š Performance Notes

On a Mac with M1/M2/M3 (Apple Silicon):
- **Indexing:** ~5-10 seconds per 10-page PDF
- **Query response:** ~2-5 seconds per question
- **RAM usage:** ~2-3GB while running

On Intel Mac:
- Indexing and queries will be slower (3-4x)
- Consider using smaller models or upgrading RAM

---

## ğŸ” Privacy & Security

- âœ… All processing happens locally
- âœ… PDFs are temporarily stored in `temp.pdf` (deleted after upload)
- âœ… Vector embeddings stay in memory (`temp.pkl` after indexing)
- âœ… No data sent to OpenAI, Google, or any cloud service
- âœ… No telemetry or usage tracking

---

## ğŸš€ Next Steps

1. **Try with your own PDFs** â€” test with documents from work or research
2. **Experiment with different models** â€” see which speed/quality tradeoff works best
3. **Integrate into production** â€” wrap this in an API or use in other applications
4. **Fine-tune prompts** â€” edit the `system_prompt` in `app.py` for better results

---

## ğŸ“š Resources

- [Ollama Documentation](https://github.com/ollama/ollama)
- [LangChain Documentation](https://python.langchain.com/)
- [Streamlit Documentation](https://docs.streamlit.io/)
- [FAISS Documentation](https://github.com/facebookresearch/faiss)
- [HuggingFace Embeddings](https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2)

---

## ğŸ“ License

This project is open source. See LICENSE file for details.

---

## ğŸ’¬ Support

For issues or questions:
1. Check the Troubleshooting section above
2. Review error messages in the terminal
3. Check browser console (F12) for UI errors
4. Ensure all prerequisites are properly installed
